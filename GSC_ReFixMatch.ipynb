{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2579be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and seeds for reproducibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e047d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as f:\n",
    "                return [os.path.join(self._path, line.strip()) for line in f]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "            \n",
    "        print(f\"Loaded {subset} set with {len(self._walker)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_set = SubsetSC(\"training\")\n",
    "val_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "# Get list of labels\n",
    "labels = sorted(list(set(item[2] for item in train_set)))\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "index_to_label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "print(f\"Number of classes: {len(labels)}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio preprocessing\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sample_rate=16000, n_mfcc=40, n_fft=400, hop_length=160):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # MFCC transform\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "                'n_fft': n_fft,\n",
    "                'hop_length': hop_length,\n",
    "                'n_mels': 80,\n",
    "                'center': False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def __call__(self, waveform, sample_rate):\n",
    "        # Resample if needed\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = T.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "        # Pad or trim to 1 second (16000 samples)\n",
    "        if waveform.shape[1] < self.sample_rate:\n",
    "            waveform = F.pad(waveform, (0, self.sample_rate - waveform.shape[1]))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.sample_rate]\n",
    "            \n",
    "        # Extract MFCC features\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        \n",
    "        # Check for NaN or Inf values\n",
    "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
    "            mfcc = torch.nan_to_num(mfcc, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "class AudioAugmentation:\n",
    "    def __init__(self):\n",
    "        self.time_mask = T.TimeMasking(time_mask_param=10)\n",
    "        self.freq_mask = T.FrequencyMasking(freq_mask_param=10)\n",
    "        \n",
    "    def weak_augment(self, x):\n",
    "        # Apply mild augmentation\n",
    "        x = self.freq_mask(x)\n",
    "        return x\n",
    "    \n",
    "    def strong_augment(self, x):\n",
    "        # Apply stronger augmentation\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        # Add some noise\n",
    "        noise = torch.randn_like(x) * 0.1\n",
    "        x = x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for semi-supervised learning\n",
    "class SpeechCommandDataset(Dataset):\n",
    "    def __init__(self, dataset, preprocessor, label_to_index):\n",
    "        self.dataset = dataset\n",
    "        self.preprocessor = preprocessor\n",
    "        self.label_to_index = label_to_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate, label, _, _ = self.dataset[idx]\n",
    "        \n",
    "        # Preprocess audio\n",
    "        features = self.preprocessor(waveform, sample_rate)\n",
    "        \n",
    "        # Convert label to index\n",
    "        label_idx = self.label_to_index[label]\n",
    "        \n",
    "        return features, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = AudioPreprocessor()\n",
    "\n",
    "# Create datasets with preprocessing\n",
    "train_dataset = SpeechCommandDataset(train_set, preprocessor, label_to_index)\n",
    "val_dataset = SpeechCommandDataset(val_set, preprocessor, label_to_index)\n",
    "test_dataset = SpeechCommandDataset(test_set, preprocessor, label_to_index)\n",
    "\n",
    "# Split training set into labeled and unlabeled\n",
    "num_labeled = 1000  # Number of labeled examples to use\n",
    "total_samples = len(train_dataset)\n",
    "\n",
    "# Create indices for the split\n",
    "indices = list(range(total_samples))\n",
    "random.shuffle(indices)\n",
    "labeled_indices = indices[:num_labeled]\n",
    "unlabeled_indices = indices[num_labeled:]\n",
    "\n",
    "# Create labeled and unlabeled datasets\n",
    "labeled_dataset = Subset(train_dataset, labeled_indices)\n",
    "unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
    "\n",
    "print(f\"Labeled samples: {len(labeled_dataset)}\")\n",
    "print(f\"Unlabeled samples: {len(unlabeled_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "labeled_batch_size = 16  # Per batch\n",
    "\n",
    "labeled_loader = DataLoader(\n",
    "    labeled_dataset,\n",
    "    batch_size=labeled_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "unlabeled_loader = DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=batch_size - labeled_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Check a batch from each loader to verify shapes\n",
    "for name, loader in [(\"Labeled\", labeled_loader), (\"Unlabeled\", unlabeled_loader), (\"Val\", val_loader)]:\n",
    "    inputs, labels = next(iter(loader))\n",
    "    print(f\"{name} batch - inputs: {inputs.shape}, labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved CNN model with residual connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioResNet, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(128, 256, 2, stride=2)\n",
    "        \n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layer\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dropout and final layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AudioResNet(num_classes=len(labels)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved ReFixMatch training function\n",
    "class ReFixMatch:\n",
    "    def __init__(self, model, num_classes, lambda_u=1.0, threshold=0.95):\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_u = lambda_u\n",
    "        self.threshold = threshold\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=2)\n",
    "        self.augmentation = AudioAugmentation()\n",
    "        \n",
    "    def train_epoch(self, labeled_loader, unlabeled_loader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_sup_loss = 0.0\n",
    "        total_unsup_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Create iterators for the loaders\n",
    "        labeled_iter = iter(labeled_loader)\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        \n",
    "        # Determine number of batches\n",
    "        num_batches = min(len(labeled_loader), len(unlabeled_loader))\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch_idx in pbar:\n",
    "            # Get labeled batch\n",
    "            try:\n",
    "                inputs_x, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, targets_x = next(labeled_iter)\n",
    "            \n",
    "            # Get unlabeled batch\n",
    "            try:\n",
    "                inputs_u, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u, _ = next(unlabeled_iter)\n",
    "            \n",
    "            # Move to device\n",
    "            inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "            inputs_u = inputs_u.to(device)\n",
    "            \n",
    "            batch_size = inputs_x.size(0)\n",
    "            \n",
    "            # Generate pseudo-labels using the model and weak augmentation\n",
    "            with torch.no_grad():\n",
    "                # Apply weak augmentation\n",
    "                inputs_u_weak = torch.stack([self.augmentation.weak_augment(x.unsqueeze(0)).squeeze(0) for x in inputs_u])\n",
    "                \n",
    "                # Get model predictions\n",
    "                outputs_u = self.model(inputs_u_weak)\n",
    "                probs_u = torch.softmax(outputs_u, dim=1)\n",
    "                \n",
    "                # Get pseudo-labels and mask based on confidence threshold\n",
    "                max_probs, pseudo_labels = torch.max(probs_u, dim=1)\n",
    "                mask = max_probs.ge(self.threshold).float()\n",
    "            \n",
    "            # Apply strong augmentation to unlabeled data\n",
    "            inputs_u_strong = torch.stack([self.augmentation.strong_augment(x.unsqueeze(0)).squeeze(0) for x in inputs_u])\n",
    "            \n",
    "            # Forward pass for all inputs\n",
    "            logits_x = self.model(inputs_x)\n",
    "            logits_u = self.model(inputs_u_strong)\n",
    "            \n",
    "            # Supervised loss\n",
    "            loss_x = self.criterion(logits_x, targets_x)\n",
    "            \n",
    "            # Unsupervised loss with pseudo-labels (only if mask has positive values)\n",
    "            if mask.sum() > 0:\n",
    "                loss_u = (F.cross_entropy(logits_u, pseudo_labels, reduction='none') * mask).mean()\n",
    "            else:\n",
    "                loss_u = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            # Ramp up unsupervised loss weight\n",
    "            # Start with a lower weight and gradually increase it\n",
    "            rampup_length = 50  # epochs\n",
    "            current_lambda_u = self.lambda_u * min(1.0, (epoch + batch_idx/num_batches) / rampup_length)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = loss_x + current_lambda_u * loss_u\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN loss detected!\")\n",
    "                print(f\"loss_x: {loss_x}, loss_u: {loss_u}\")\n",
    "                print(f\"logits_x min/max: {logits_x.min()}/{logits_x.max()}\")\n",
    "                print(f\"logits_u min/max: {logits_u.min()}/{logits_u.max()}\")\n",
    "                # Skip this batch\n",
    "                continue\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            total_sup_loss += loss_x.item()\n",
    "            total_unsup_loss += loss_u.item()\n",
    "            \n",
    "            # Calculate accuracy for supervised data\n",
    "            _, predicted = torch.max(logits_x, 1)\n",
    "            total += targets_x.size(0)\n",
    "            correct += (predicted == targets_x).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'sup_loss': f\"{loss_x.item():.4f}\",\n",
    "                'unsup_loss': f\"{loss_u.item():.4f}\",\n",
    "                'mask': f\"{mask.mean().item():.2f}\",\n",
    "                'acc': f\"{100 * correct / total:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        # Update learning rate\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_sup_loss = total_sup_loss / num_batches\n",
    "        avg_unsup_loss = total_unsup_loss / num_batches\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        return avg_loss, avg_sup_loss, avg_unsup_loss, accuracy\n",
    "    \n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                # Update accuracy\n",
    "                total += targets.size(0)\n",
    "                correct += (preds == targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, labeled_loader, unlabeled_loader, val_loader, num_epochs=100):\n",
    "        best_accuracy = 0.0\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train for one epoch\n",
    "            train_loss, sup_loss, unsup_loss, train_acc = self.train_epoch(\n",
    "                labeled_loader, unlabeled_loader, epoch\n",
    "            )\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_loss, val_accuracy = self.evaluate(val_loader)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Sup Loss: {sup_loss:.4f} | \"\n",
    "                  f\"Unsup Loss: {unsup_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Acc: {val_accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'accuracy': val_accuracy,\n",
    "                }, 'GSC_ReFix.pt')\n",
    "                print(f\"New best model saved with accuracy: {val_accuracy*100:.2f}%\")\n",
    "        \n",
    "        # Plot training progress\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(val_accuracies)\n",
    "        plt.title('Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_progress.png')\n",
    "        plt.show()\n",
    "        \n",
    "        return best_accuracy, train_losses, val_accuracies\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        # Load best model\n",
    "        checkpoint = torch.load('GSC_ReFix.pt')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_accuracy = self.evaluate(test_loader)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Computing confusion matrix\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Get unique labels that actually appear in the test set\n",
    "        unique_labels = sorted(set(all_targets))\n",
    "        label_names = [index_to_label.get(i, f\"Unknown-{i}\") for i in unique_labels]\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(\n",
    "            all_targets, \n",
    "            all_preds, \n",
    "            labels=unique_labels,\n",
    "            target_names=label_names,\n",
    "            digits=3\n",
    "        )\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(all_targets, all_preds, labels=unique_labels)\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        sns.heatmap(\n",
    "            cm, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cmap='Blues',\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.show()\n",
    "        \n",
    "        return test_accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ReFixMatch trainer with improved parameters\n",
    "trainer = ReFixMatch(\n",
    "    model=model,\n",
    "    num_classes=len(labels),\n",
    "    lambda_u=3.0,  # Increased weight for unsupervised loss\n",
    "    threshold=0.8  # Lowered confidence threshold for pseudo-labeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting ReFixMatch training...\")\n",
    "num_epochs = 100  # Increased number of epochs\n",
    "best_accuracy, train_losses, val_accuracies = trainer.train(\n",
    "    labeled_loader=labeled_loader,\n",
    "    unlabeled_loader=unlabeled_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Best validation accuracy: {best_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb265f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_accuracy, test_report = trainer.test(test_loader)\n",
    "print(f\"Final test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
